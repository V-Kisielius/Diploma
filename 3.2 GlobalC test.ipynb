{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected devie is cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import MyData\n",
    "from model import Net\n",
    "from config import device\n",
    "from helper import downscale_map, open_img_as_array\n",
    "\n",
    "PATH_TO_DATASET = './CloC/'\n",
    "IMG_SIZE = (217, 334)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [9:16:13<00:00, 333.74s/it] \n"
     ]
    }
   ],
   "source": [
    "with open('results2.txt', 'w') as f:\n",
    "    for path_to_img in tqdm(glob(PATH_TO_DATASET + '*.png')[:100]):\n",
    "        img_name = path_to_img[len(PATH_TO_DATASET):]\n",
    "        f.write(f'{img_name}\\n')\n",
    "        normal_img_array = 1 - open_img_as_array(path_to_img) // 255\n",
    "        steps = [20, 15, 10, 1]\n",
    "        map_cascade = [downscale_map(normal_img_array, step)\n",
    "                       for step in steps]\n",
    "\n",
    "        for i, cas_map in enumerate(map_cascade):\n",
    "            dataset = MyData(path_or_img=255 * cas_map, data_mode='img',\n",
    "                             mode_3d='cylinder', radius=1, reduce_fctor=1, need_info=False)\n",
    "            dataset_list = [dataset]\n",
    "            model = Net(dataset_list=dataset_list, lr=1e-3)\n",
    "            model.to(device)\n",
    "            if i:\n",
    "                model.load_state_dict(torch.load(\n",
    "                    f'./state_dict/GlobalCas/{inum}/{i}_CloC.pt'))\n",
    "            model.start_training(\n",
    "                num_epochs=15e+3, my_weight=0.1, need_plot=False, need_save=False)\n",
    "            inum = img_name.split('.')[0]\n",
    "            model.save_state_dict(\n",
    "                f'./state_dict/GlobalCas/{inum}/{i+1}_CloC.pt')\n",
    "        mse, f1, f2 = model.test_model_(threshold=0.15)\n",
    "        f.write(f'MSE: {mse:.2f} F1: {f1:.2f}, {f2:.2f}\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "850244c8638196155f69ccd644181272b4a42cc2d794d5c02d6df1253ef9f395"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
